# 什么是Transformer？所有的神经网络都基于矩阵计算，Transformer哪里特殊能够在NLP任务上出类拔萃？
* Transformer 是一种用于处理序列数据的神经网络架构（比如一句话由多个词组成，就是序列）。

它最初用于 自然语言处理（NLP），比如机器翻译（英文 → 中文），但现在已经成为 AI 的基础，广泛用于：

ChatGPT、LLM（大模型）

图片模型（Vision Transformer）

多模态模型

音频、推荐系统、基因序列分析……

一句话概括：

Transformer 是一种用“注意力机制（Attention）”来理解序列的模型。
* 但随着Dennard定律在微观维度不再生效 [^量子隧穿]，我们很难高性价比的继续完成通用计算单元的升级，随着大数据、AI的发展需要，并发专精的GPU便蓬勃发展；
* 但是我们不可能只靠GPU搭建一套合理的系统，我们也终于从原先的CPU通用计算演进到“异构计算”，即同时使用 CPU + GPU（可能还有 ASIC、FPGA 等）来匹配任务特性，实现更高效的资源利用。


# GPU怎么做到比CPU更快？
这就涉及到GPU的设计理念的核心 ——“以吞吐量换取延迟”。
* GPU（图形处理器）最初是为了同时处理成千上万个像素点而设计的，像素之间几乎没有依赖关系，计算公式也一样，所以GPU能够进行大量相同或相似计算的并行执行（例如矩阵乘法、卷积、光照计算）。
* CPU则一开始就需要处理复杂逻辑、分支多、任务差异大的指令（比如操作系统调度、网页渲染、程序控制流等），将大量的资源用在延迟处理上。


所以CPU是串行的处理复杂逻辑的算力，单次处理快但成本高，核心是处理因为自己太快导致的延迟问题；而GPU是牺牲灵活性与延迟为代价换来更高的处理速度的算力，两者相辅相成。不是说CPU不能用来做大模型计算，只是成本太高。

| 特性    | CPU           | GPU              |
| ----- | ------------- | ---------------- |
| 主要目标  | 延迟最小          | 吞吐量最大            |
| 核心数量  | 少（4–64）       | 多（上千）            |
| 每核复杂度 | 高（控制强、cache大） | 低（控制弱、算力密集）      |
| 并行方式  | 多任务并行（MIMD）   | 大规模数据并行（SIMT）    |
| 内存延迟  | 低             | 高（但能隐藏）          |
| 适合任务  | 分支复杂、逻辑控制     | 计算密集、数据并行        |
| 示例任务  | 浏览器、数据库、操作系统  | 神经网络训练、图像处理、矩阵乘法 |



![img_4.png](../resource/img_4.png)


[^Dennard定律]: 当晶体管特征尺寸缩小时，其功率密度保持恒定。（晶体管变小 → 电压变低 → 发热不变 → 可以塞更多 → 性能提升）
[^量子隧穿]: 当晶体管变得非常非常小（纳米级别，约是头发丝直径的十万分之一）时，电子开始“不老实”了，就像墙变得太薄，小球能“穿过去”一样，电子也能穿过晶体管的绝缘层，即使电源没开，它也偷偷“漏电”，这就是所谓的 量子隧穿效应（Quantum Tunneling）。
